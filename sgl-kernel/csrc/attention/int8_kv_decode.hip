// !!! This is a file automatically generated by hipify!!!
#include <torch/extension.h>

#if defined(__HIPCC__)
#include <hip/hip_runtime.h>
#include <hip/hip_runtime_api.h>
#else
#include <hip/hip_runtime.h>
#endif

#include <ATen/hip/HIPContext.h>
#include <c10/hip/HIPGuard.h>

#include <type_traits>

namespace {

constexpr int kWarpSize = 32;
constexpr int kMaxVHeadDim = 1024;
constexpr int kMaxVPerLane = (kMaxVHeadDim + kWarpSize - 1) / kWarpSize;

__device__ __forceinline__ float warp_reduce_sum(float val) {
#if defined(__HIPCC__)
  for (int offset = kWarpSize / 2; offset > 0; offset >>= 1) {
    val += __shfl_down(val, offset, kWarpSize);
  }
#else
  for (int offset = kWarpSize / 2; offset > 0; offset >>= 1) {
    val += __shfl_down_sync(0xffffffff, val, offset);
  }
#endif
  return val;
}

__device__ __forceinline__ float warp_reduce_max(float val) {
#if defined(__HIPCC__)
  for (int offset = kWarpSize / 2; offset > 0; offset >>= 1) {
    val = fmaxf(val, __shfl_down(val, offset, kWarpSize));
  }
#else
  for (int offset = kWarpSize / 2; offset > 0; offset >>= 1) {
    val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));
  }
#endif
  return val;
}

template <typename T>
__device__ __forceinline__ float to_float(T val) {
  return static_cast<float>(val);
}

template <>
__device__ __forceinline__ float to_float<half>(half val) {
  return __half2float(val);
}

template <typename Q, typename Out>
__global__ void decode_int8_kv_kernel(
    const Q* __restrict__ q,
    const int8_t* __restrict__ k_cache,
    const int8_t* __restrict__ v_cache,
    const half* __restrict__ k_scale,
    const half* __restrict__ v_scale,
    const int32_t* __restrict__ kv_indptr,
    const int32_t* __restrict__ kv_indices,
    Out* __restrict__ out,
    int32_t batch_size,
    int32_t head_num,
    int32_t kv_head_num,
    int32_t qk_head_dim,
    int32_t v_head_dim,
    int32_t kv_group_size,
    int32_t max_tokens,
    int64_t stride_qbs,
    int64_t stride_qh,
    int64_t stride_kbs,
    int64_t stride_kh,
    int64_t stride_vbs,
    int64_t stride_vh,
    int64_t stride_ks,
    int64_t stride_ksh,
    int64_t stride_vs,
    int64_t stride_vsh,
    int64_t stride_obs,
    int64_t stride_oh,
    float sm_scale,
    float logit_cap) {
  int b = blockIdx.x;
  int h = blockIdx.y;
  if (b >= batch_size || h >= head_num) {
    return;
  }

  if (kv_head_num <= 0) {
    return;
  }
  int kv_group_num = head_num / kv_head_num;
  if (kv_group_num <= 0) {
    return;
  }
  int kv_head = h / kv_group_num;
  if (kv_head >= kv_head_num) {
    return;
  }

  int lane = threadIdx.x;
  if (lane >= kWarpSize) {
    return;
  }

  extern __shared__ char smem[];
  float* q_shared = reinterpret_cast<float*>(smem);
  float acc[kMaxVPerLane];

  // Load q into shared memory.
  for (int d = lane; d < qk_head_dim; d += kWarpSize) {
    q_shared[d] = to_float(q[b * stride_qbs + h * stride_qh + d]);
  }

#if defined(__HIPCC__)
  __syncthreads();
#else
  __syncthreads();
#endif

  if (v_head_dim > kMaxVHeadDim) {
    return;
  }

  int v_per_lane = (v_head_dim + kWarpSize - 1) / kWarpSize;
  for (int i = 0; i < v_per_lane; ++i) {
    acc[i] = 0.0f;
  }

  int32_t kv_start = kv_indptr[b];
  int32_t kv_end = kv_indptr[b + 1];

  float m = -INFINITY;
  float l = 0.0f;

  for (int32_t kv_idx = kv_start; kv_idx < kv_end; ++kv_idx) {
    int32_t token_idx = kv_indices[kv_idx];
    bool token_valid = token_idx >= 0 && token_idx < max_tokens;

    float qk = -INFINITY;
    if (token_valid) {
      float partial = 0.0f;
      for (int d = lane; d < qk_head_dim; d += kWarpSize) {
        int scale_idx = d / kv_group_size;
        float scale = __half2float(
            k_scale[token_idx * stride_ks + kv_head * stride_ksh + scale_idx]);
        int8_t k_val = k_cache[token_idx * stride_kbs + kv_head * stride_kh + d];
        partial += q_shared[d] * (static_cast<float>(k_val) * scale);
      }
      qk = warp_reduce_sum(partial) * sm_scale;
      if (logit_cap > 0.0f) {
        qk = logit_cap * tanhf(qk / logit_cap);
      }
    }

    float m_new = fmaxf(m, qk);
    float exp_scale = (m == -INFINITY) ? 0.0f : expf(m - m_new);
    float p = token_valid ? expf(qk - m_new) : 0.0f;
    float l_new = l * exp_scale + p;

    for (int i = 0; i < v_per_lane; ++i) {
      acc[i] *= exp_scale;
    }

    if (token_valid && p != 0.0f) {
      int acc_idx = 0;
      for (int d = lane; d < v_head_dim; d += kWarpSize, ++acc_idx) {
        int scale_idx = d / kv_group_size;
        float scale = __half2float(
            v_scale[token_idx * stride_vs + kv_head * stride_vsh + scale_idx]);
        int8_t v_val = v_cache[token_idx * stride_vbs + kv_head * stride_vh + d];
        acc[acc_idx] += p * (static_cast<float>(v_val) * scale);
      }
    }

    m = m_new;
    l = l_new;
  }

  float inv_l = (l > 0.0f) ? (1.0f / l) : 0.0f;
  int acc_idx = 0;
  for (int d = lane; d < v_head_dim; d += kWarpSize, ++acc_idx) {
    float out_val = acc[acc_idx] * inv_l;
    if constexpr (std::is_same<Out, half>::value) {
      out[b * stride_obs + h * stride_oh + d] = __float2half(out_val);
    } else {
      out[b * stride_obs + h * stride_oh + d] = out_val;
    }
  }
}

}  // namespace

void decode_attention_int8_kv(
    at::Tensor q,
    at::Tensor k_cache,
    at::Tensor v_cache,
    at::Tensor k_scale,
    at::Tensor v_scale,
    at::Tensor kv_indptr,
    at::Tensor kv_indices,
    at::Tensor output,
    double sm_scale,
    double logit_cap,
    int64_t kv_group_size) {
  const auto batch_size = q.size(0);
  const auto head_num = q.size(1);
  const auto qk_head_dim = q.size(2);
  const auto v_head_dim = v_cache.size(2);
  const auto kv_head_num = k_cache.size(1);
  const auto max_tokens = k_cache.size(0);

  TORCH_CHECK(q.is_cuda(), "q must be CUDA/HIP");
  TORCH_CHECK(k_cache.is_cuda(), "k_cache must be CUDA/HIP");
  TORCH_CHECK(v_cache.is_cuda(), "v_cache must be CUDA/HIP");
  TORCH_CHECK(k_scale.is_cuda(), "k_scale must be CUDA/HIP");
  TORCH_CHECK(v_scale.is_cuda(), "v_scale must be CUDA/HIP");
  TORCH_CHECK(output.is_cuda(), "output must be CUDA/HIP");
  TORCH_CHECK(kv_indptr.is_cuda(), "kv_indptr must be CUDA/HIP");
  TORCH_CHECK(kv_indices.is_cuda(), "kv_indices must be CUDA/HIP");
  TORCH_CHECK(
      q.scalar_type() == at::kHalf || q.scalar_type() == at::kFloat,
      "q must be float16 or float32");
  TORCH_CHECK(k_cache.scalar_type() == at::kChar, "k_cache must be int8");
  TORCH_CHECK(v_cache.scalar_type() == at::kChar, "v_cache must be int8");
  TORCH_CHECK(k_scale.scalar_type() == at::kHalf, "k_scale must be float16");
  TORCH_CHECK(v_scale.scalar_type() == at::kHalf, "v_scale must be float16");
  TORCH_CHECK(
      output.scalar_type() == at::kHalf || output.scalar_type() == at::kFloat,
      "output must be float16 or float32");
  TORCH_CHECK(kv_indptr.scalar_type() == at::kInt, "kv_indptr must be int32");
  TORCH_CHECK(kv_indices.scalar_type() == at::kInt, "kv_indices must be int32");
  TORCH_CHECK(
      qk_head_dim % kv_group_size == 0,
      "qk_head_dim must be divisible by kv_group_size");
  TORCH_CHECK(
      v_head_dim % kv_group_size == 0,
      "v_head_dim must be divisible by kv_group_size");
  TORCH_CHECK(
      v_head_dim <= kMaxVHeadDim,
      "v_head_dim exceeds kernel limit");

  const dim3 grid(batch_size, head_num, 1);
  const dim3 block(kWarpSize, 1, 1);
  size_t shared_bytes =
      sizeof(float) * qk_head_dim + sizeof(float) * v_head_dim;

  const auto stream = at::cuda::getDefaultCUDAStream();

  const bool q_is_fp16 = q.scalar_type() == at::kHalf;
  const bool out_is_fp16 = output.scalar_type() == at::kHalf;
  if (!out_is_fp16 && q_is_fp16) {
    TORCH_CHECK(false, "float32 output requires float32 q");
  }
#if defined(__HIPCC__)
  if (out_is_fp16 && q_is_fp16) {
    hipLaunchKernelGGL(
        (decode_int8_kv_kernel<half, half>),
        grid,
        block,
        shared_bytes,
        stream,
        reinterpret_cast<half*>(q.data_ptr<at::Half>()),
        reinterpret_cast<int8_t*>(k_cache.data_ptr<int8_t>()),
        reinterpret_cast<int8_t*>(v_cache.data_ptr<int8_t>()),
        reinterpret_cast<half*>(k_scale.data_ptr<at::Half>()),
        reinterpret_cast<half*>(v_scale.data_ptr<at::Half>()),
        reinterpret_cast<int32_t*>(kv_indptr.data_ptr<int32_t>()),
        reinterpret_cast<int32_t*>(kv_indices.data_ptr<int32_t>()),
        reinterpret_cast<half*>(output.data_ptr<at::Half>()),
        static_cast<int32_t>(batch_size),
        static_cast<int32_t>(head_num),
        static_cast<int32_t>(kv_head_num),
        static_cast<int32_t>(qk_head_dim),
        static_cast<int32_t>(v_head_dim),
        static_cast<int32_t>(kv_group_size),
        static_cast<int32_t>(max_tokens),
        q.stride(0),
        q.stride(1),
        k_cache.stride(0),
        k_cache.stride(1),
        v_cache.stride(0),
        v_cache.stride(1),
        k_scale.stride(0),
        k_scale.stride(1),
        v_scale.stride(0),
        v_scale.stride(1),
        output.stride(0),
        output.stride(1),
        static_cast<float>(sm_scale),
        static_cast<float>(logit_cap));
  } else if (out_is_fp16 && !q_is_fp16) {
    hipLaunchKernelGGL(
        (decode_int8_kv_kernel<float, half>),
        grid,
        block,
        shared_bytes,
        stream,
        reinterpret_cast<float*>(q.data_ptr<float>()),
        reinterpret_cast<int8_t*>(k_cache.data_ptr<int8_t>()),
        reinterpret_cast<int8_t*>(v_cache.data_ptr<int8_t>()),
        reinterpret_cast<half*>(k_scale.data_ptr<at::Half>()),
        reinterpret_cast<half*>(v_scale.data_ptr<at::Half>()),
        reinterpret_cast<int32_t*>(kv_indptr.data_ptr<int32_t>()),
        reinterpret_cast<int32_t*>(kv_indices.data_ptr<int32_t>()),
        reinterpret_cast<half*>(output.data_ptr<at::Half>()),
        static_cast<int32_t>(batch_size),
        static_cast<int32_t>(head_num),
        static_cast<int32_t>(kv_head_num),
        static_cast<int32_t>(qk_head_dim),
        static_cast<int32_t>(v_head_dim),
        static_cast<int32_t>(kv_group_size),
        static_cast<int32_t>(max_tokens),
        q.stride(0),
        q.stride(1),
        k_cache.stride(0),
        k_cache.stride(1),
        v_cache.stride(0),
        v_cache.stride(1),
        k_scale.stride(0),
        k_scale.stride(1),
        v_scale.stride(0),
        v_scale.stride(1),
        output.stride(0),
        output.stride(1),
        static_cast<float>(sm_scale),
        static_cast<float>(logit_cap));
  } else {
    hipLaunchKernelGGL(
        (decode_int8_kv_kernel<float, float>),
        grid,
        block,
        shared_bytes,
        stream,
        reinterpret_cast<float*>(q.data_ptr<float>()),
        reinterpret_cast<int8_t*>(k_cache.data_ptr<int8_t>()),
        reinterpret_cast<int8_t*>(v_cache.data_ptr<int8_t>()),
        reinterpret_cast<half*>(k_scale.data_ptr<at::Half>()),
        reinterpret_cast<half*>(v_scale.data_ptr<at::Half>()),
        reinterpret_cast<int32_t*>(kv_indptr.data_ptr<int32_t>()),
        reinterpret_cast<int32_t*>(kv_indices.data_ptr<int32_t>()),
        reinterpret_cast<float*>(output.data_ptr<float>()),
        static_cast<int32_t>(batch_size),
        static_cast<int32_t>(head_num),
        static_cast<int32_t>(kv_head_num),
        static_cast<int32_t>(qk_head_dim),
        static_cast<int32_t>(v_head_dim),
        static_cast<int32_t>(kv_group_size),
        static_cast<int32_t>(max_tokens),
        q.stride(0),
        q.stride(1),
        k_cache.stride(0),
        k_cache.stride(1),
        v_cache.stride(0),
        v_cache.stride(1),
        k_scale.stride(0),
        k_scale.stride(1),
        v_scale.stride(0),
        v_scale.stride(1),
        output.stride(0),
        output.stride(1),
        static_cast<float>(sm_scale),
        static_cast<float>(logit_cap));
  }
#else
  if (out_is_fp16 && q_is_fp16) {
   hipLaunchKernelGGL(( decode_int8_kv_kernel<half, half>), dim3(grid), dim3(block), shared_bytes, stream, 
        reinterpret_cast<half*>(q.data_ptr<at::Half>()),
        reinterpret_cast<int8_t*>(k_cache.data_ptr<int8_t>()),
        reinterpret_cast<int8_t*>(v_cache.data_ptr<int8_t>()),
        reinterpret_cast<half*>(k_scale.data_ptr<at::Half>()),
        reinterpret_cast<half*>(v_scale.data_ptr<at::Half>()),
        reinterpret_cast<int32_t*>(kv_indptr.data_ptr<int32_t>()),
        reinterpret_cast<int32_t*>(kv_indices.data_ptr<int32_t>()),
        reinterpret_cast<half*>(output.data_ptr<at::Half>()),
        static_cast<int32_t>(batch_size),
        static_cast<int32_t>(head_num),
        static_cast<int32_t>(kv_head_num),
        static_cast<int32_t>(qk_head_dim),
        static_cast<int32_t>(v_head_dim),
        static_cast<int32_t>(kv_group_size),
        static_cast<int32_t>(max_tokens),
        q.stride(0),
        q.stride(1),
        k_cache.stride(0),
        k_cache.stride(1),
        v_cache.stride(0),
        v_cache.stride(1),
        k_scale.stride(0),
        k_scale.stride(1),
        v_scale.stride(0),
        v_scale.stride(1),
        output.stride(0),
        output.stride(1),
        static_cast<float>(sm_scale),
        static_cast<float>(logit_cap));
  } else if (out_is_fp16 && !q_is_fp16) {
   hipLaunchKernelGGL(( decode_int8_kv_kernel<float, half>), dim3(grid), dim3(block), shared_bytes, stream, 
        reinterpret_cast<float*>(q.data_ptr<float>()),
        reinterpret_cast<int8_t*>(k_cache.data_ptr<int8_t>()),
        reinterpret_cast<int8_t*>(v_cache.data_ptr<int8_t>()),
        reinterpret_cast<half*>(k_scale.data_ptr<at::Half>()),
        reinterpret_cast<half*>(v_scale.data_ptr<at::Half>()),
        reinterpret_cast<int32_t*>(kv_indptr.data_ptr<int32_t>()),
        reinterpret_cast<int32_t*>(kv_indices.data_ptr<int32_t>()),
        reinterpret_cast<half*>(output.data_ptr<at::Half>()),
        static_cast<int32_t>(batch_size),
        static_cast<int32_t>(head_num),
        static_cast<int32_t>(kv_head_num),
        static_cast<int32_t>(qk_head_dim),
        static_cast<int32_t>(v_head_dim),
        static_cast<int32_t>(kv_group_size),
        static_cast<int32_t>(max_tokens),
        q.stride(0),
        q.stride(1),
        k_cache.stride(0),
        k_cache.stride(1),
        v_cache.stride(0),
        v_cache.stride(1),
        k_scale.stride(0),
        k_scale.stride(1),
        v_scale.stride(0),
        v_scale.stride(1),
        output.stride(0),
        output.stride(1),
        static_cast<float>(sm_scale),
        static_cast<float>(logit_cap));
  } else {
   hipLaunchKernelGGL(( decode_int8_kv_kernel<float, float>), dim3(grid), dim3(block), shared_bytes, stream, 
        reinterpret_cast<float*>(q.data_ptr<float>()),
        reinterpret_cast<int8_t*>(k_cache.data_ptr<int8_t>()),
        reinterpret_cast<int8_t*>(v_cache.data_ptr<int8_t>()),
        reinterpret_cast<half*>(k_scale.data_ptr<at::Half>()),
        reinterpret_cast<half*>(v_scale.data_ptr<at::Half>()),
        reinterpret_cast<int32_t*>(kv_indptr.data_ptr<int32_t>()),
        reinterpret_cast<int32_t*>(kv_indices.data_ptr<int32_t>()),
        reinterpret_cast<float*>(output.data_ptr<float>()),
        static_cast<int32_t>(batch_size),
        static_cast<int32_t>(head_num),
        static_cast<int32_t>(kv_head_num),
        static_cast<int32_t>(qk_head_dim),
        static_cast<int32_t>(v_head_dim),
        static_cast<int32_t>(kv_group_size),
        static_cast<int32_t>(max_tokens),
        q.stride(0),
        q.stride(1),
        k_cache.stride(0),
        k_cache.stride(1),
        v_cache.stride(0),
        v_cache.stride(1),
        k_scale.stride(0),
        k_scale.stride(1),
        v_scale.stride(0),
        v_scale.stride(1),
        output.stride(0),
        output.stride(1),
        static_cast<float>(sm_scale),
        static_cast<float>(logit_cap));
  }
#endif
}
